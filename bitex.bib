@inproceedings{rsglue,
    title = "{R}ussian{S}uper{GLUE}: A {R}ussian Language Understanding Evaluation Benchmark",
    author = "Shavrina, Tatiana  and
      Fenogenova, Alena  and
      Anton, Emelyanov  and
      Shevelev, Denis  and
      Artemova, Ekaterina  and
      Malykh, Valentin  and
      Mikhailov, Vladislav  and
      Tikhonova, Maria  and
      Chertok, Andrey  and
      Evlampiev, Andrey",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.381/",
    doi = "10.18653/v1/2020.emnlp-main.381",
    pages = "4717--4726",
    abstract = "In this paper, we introduce an advanced Russian general language understanding evaluation benchmark {--} Russian SuperGLUE. Recent advances in the field of universal language models and transformers require the development of a methodology for their broad diagnostics and testing for general intellectual skills - detection of natural language inference, commonsense reasoning, ability to perform simple logical operations regardless of text subject or lexicon. For the first time, a benchmark of nine tasks, collected and organized analogically to the SuperGLUE methodology, was developed from scratch for the Russian language. We also provide baselines, human level evaluation, open-source framework for evaluating models, and an overall leaderboard of transformer models for the Russian language. Besides, we present the first results of comparing multilingual models in the translated diagnostic test set and offer the first steps to further expanding or assessing State-of-the-art models independently of language."
}

@inproceedings{mera,
    title = "{MERA}: A Comprehensive {LLM} Evaluation in {R}ussian",
    author = "Fenogenova, Alena  and
      Chervyakov, Artem  and
      Martynov, Nikita  and
      Kozlova, Anastasia  and
      Tikhonova, Maria  and
      Akhmetgareeva, Albina  and
      Emelyanov, Anton  and
      Shevelev, Denis  and
      Lebedev, Pavel  and
      Sinev, Leonid  and
      Isaeva, Ulyana  and
      Kolomeytseva, Katerina  and
      Moskovskiy, Daniil  and
      Goncharova, Elizaveta  and
      Savushkin, Nikita  and
      Mikhailova, Polina  and
      Minaeva, Anastasia  and
      Dimitrov, Denis  and
      Panchenko, Alexander  and
      Markov, Sergey",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.534/",
    doi = "10.18653/v1/2024.acl-long.534",
    pages = "9920--9948",
    abstract = "Over the past few years, one of the most notable advancements in AI research has been in foundation models (FMs), headlined by the rise of language models (LMs). However, despite researchers' attention and the rapid growth in LM application, the capabilities, limitations, and associated risks still need to be better understood. To address these issues, we introduce a new instruction benchmark, MERA, oriented towards the FMs' performance on the Russian language. The benchmark encompasses 21 evaluation tasks for generative models covering 10 skills and is supplied with private answer scoring to prevent data leakage. The paper introduces a methodology to evaluate FMs and LMs in fixed zero- and few-shot instruction settings that can be extended to other modalities. We propose an evaluation methodology, an open-source code base for the MERA assessment, and a leaderboard with a submission system. We evaluate open LMs as baselines and find they are still far behind the human level. We publicly release MERA to guide forthcoming research, anticipate groundbreaking model features, standardize the evaluation procedure, and address potential ethical concerns and drawbacks."
}

@article{libra,
  author    = {Churin and others},
  title     = {"Long Input Benchmark for Russian Analysis."},
  journal   = {CoRR},
  year      = {2024}
}

@misc{arena,
  author       = {{VikhrModels}},
  title        = {RuLLM Arena: Russian LLM Evaluation Benchmark},
  year         = {2024},
  howpublished = {\url{https://github.com/VikhrModels/ru_llm_arena}},
  note         = {Accessed: 2025-07-30}
}

@misc{pp,
      title={PingPong: A Benchmark for Role-Playing Language Models with User Emulation and Multi-Model Evaluation}, 
      author={Ilya Gusev},
      year={2025},
      eprint={2409.06820},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.06820}, 
}

@misc{deepr,
  author       = {{OpenAI}},
  title        = {Introducing deep research},
  year         = {2024},
  howpublished = {\url{https://openai.com/index/introducing-deep-research/}},
  note         = {Accessed: 2025-07-30}
}

@misc{storm,
      title={Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models}, 
      author={Yijia Shao and Yucheng Jiang and Theodore A. Kanell and Peter Xu and Omar Khattab and Monica S. Lam},
      year={2024},
      eprint={2402.14207},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.14207}, 
}

@misc{resar,
      title={ResearchArena: Benchmarking Large Language Models' Ability to Collect and Organize Information as Research Agents}, 
      author={Hao Kang and Chenyan Xiong},
      year={2025},
      eprint={2406.10291},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2406.10291}, 
}

@inproceedings{rerank,
    title = "Searching for Best Practices in Retrieval-Augmented Generation",
    author = "Wang, Xiaohua  and
      Wang, Zhenghua  and
      Gao, Xuan  and
      Zhang, Feiran  and
      Wu, Yixin  and
      Xu, Zhibo  and
      Shi, Tianyuan  and
      Wang, Zhengyuan  and
      Li, Shizheng  and
      Qian, Qi  and
      Yin, Ruicheng  and
      Lv, Changze  and
      Zheng, Xiaoqing  and
      Huang, Xuanjing",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.981/",
    doi = "10.18653/v1/2024.emnlp-main.981",
    pages = "17716--17736",
    abstract = "Retrieval-augmented generation (RAG) techniques have proven to be effective in integrating up-to-date information, mitigating hallucinations, and enhancing response quality, particularly in specialized domains. While many RAG approaches have been proposed to enhance large language models through query-dependent retrievals, these approaches still suffer from their complex implementation and prolonged response times. Typically, a RAG workflow involves multiple processing steps, each of which can be executed in various ways. Here, we investigate existing RAG approaches and their potential combinations to identify optimal RAG practices. Through extensive experiments, we suggest several strategies for deploying RAG that balance both performance and efficiency. Moreover, we demonstrate that multimodal retrieval techniques can significantly enhance question-answering capabilities about visual inputs and accelerate the generation of multimodal content using a ``retrieval as generation'' strategy."
}

@misc{llama,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.13971}, 
}

@misc{hier,
      title={Recursively Summarizing Books with Human Feedback}, 
      author={Jeff Wu and Long Ouyang and Daniel M. Ziegler and Nisan Stiennon and Ryan Lowe and Jan Leike and Paul Christiano},
      year={2021},
      eprint={2109.10862},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2109.10862}, 
}

@article{ndcg,
  title={Cumulated gain-based evaluation of IR techniques},
  author={Kalervo J{\"a}rvelin and Jaana Kek{\"a}l{\"a}inen},
  journal={ACM Trans. Inf. Syst.},
  year={2002},
  volume={20},
  pages={422-446},
  url={https://api.semanticscholar.org/CorpusID:1981391}
}

@article{rprecision,
author = {Buckley, Chris and Voorhees, Ellen},
year = {2000},
month = {10},
pages = {},
title = {Evaluating Evaluation Measure Stability},
journal = {SIGIR Forum (ACM Special Interest Group on Information Retrieval)},
doi = {10.1145/345508.345543}
}

@inproceedings{bertscore,
  author    = {Tianyi Zhang and Varsha Kishore and Felix Wu and Kilian Q. Weinberger and Yoav Artzi},
  title     = {{BERTScore}: Evaluating Text Generation with {BERT}},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2020},
  url       = {https://openreview.net/forum?id=SkeHuCVFDr},
  eprint    = {1904.09675},
  archivePrefix = {arXiv}
}

@inproceedings{rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W04-1013/",
    pages = "74--81"
}

@inproceedings{bleu,
    title = "{B}leu: a Method for Automatic Evaluation of Machine Translation",
    author = "Papineni, Kishore  and
      Roukos, Salim  and
      Ward, Todd  and
      Zhu, Wei-Jing",
    editor = "Isabelle, Pierre  and
      Charniak, Eugene  and
      Lin, Dekang",
    booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2002",
    address = "Philadelphia, Pennsylvania, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P02-1040/",
    doi = "10.3115/1073083.1073135",
    pages = "311--318"
}

@article{ruadapt, title={Facilitating Large Language Model Russian Adaptation with Learned Embedding Propagation}, volume={10}, url={https://jle.hse.ru/article/view/22224}, DOI={10.17323/jle.2024.22224}, abstractNote={&lt;p&gt;&lt;strong&gt;Background: &lt;/strong&gt;Recent advancements in large language model (LLM) technologies have introduced powerful open-source instruction-tuned LLMs that match the text generation quality of leading models like GPT-4. Despite accelerating LLM adoption in sensitive-information environments, the lack of disclosed training data hinders replication and makes these achievements exclusive to specific models.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Purpose: &lt;/strong&gt;Given the multilingual nature of the latest iteration of open-source LLMs, the benefits of training language-specific LLMs diminish, leaving computational efficiency as the sole guaranteed advantage of this computationally-expensive procedure. This work aims to address the language-adaptation limitations posed by restricted access to high-quality instruction-tuning data, offering a more cost-effective pipeline.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Method: &lt;/strong&gt;To tackle language-adaptation challenges, we introduce Learned Embedding Propagation (LEP), a novel method with lower training data requirements and minimal disruption of existing LLM knowledge. LEP employs an innovative embedding propagation technique, bypassing the need for instruction-tuning and directly integrating new language knowledge into any instruct-tuned LLM variant. Additionally, we developed Darumeru, a new benchmark for evaluating text generation robustness during training, specifically tailored for Russian adaptation.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results: &lt;/strong&gt;We applied the LEP method to adapt LLaMa-3-8B and Mistral-7B for Russian, testing four different vocabulary adaptation scenarios. Evaluation demonstrates that LEP achieves competitive performance levels, comparable to OpenChat 3.5 and LLaMa-3-8B-Instruct. Further improvements were observed through self-calibration and additional instruction-tuning steps, enhancing task-solving capabilities beyond the original models.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Conclusion: &lt;/strong&gt;LEP offers a viable and efficient alternative to traditional language-specific instruction-tuning, significantly reducing the costs associated with language adaptation while maintaining or surpassing the performance benchmarks set by contemporary LLMs.&lt;/p&#38;gt;}, number={4}, journal={Journal of Language and Education}, author={Tikhomirov, Mikhail and Chernyshov, Daniil}, year={2024}, month={Dec.}, pages={130-145} }

@article{deepseek,
  author    = {Aixin Liu and others},
  title     = {DeepSeek{-}V3 Technical Report},
  journal   = {CoRR},
  year      = {2024}
}

@misc{qwen3,
      title={Qwen3 Technical Report}, 
      author={An Yang and others},
      year={2025},
      eprint={2505.09388},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2505.09388}, 
}

@misc{tpro,
  author       = {{T{-}Bank}},
  title        = {T{-}Bank has opened access to its own Russian{-}language language model in the 7--8 billion parameter weight category},
  year         = {2024},
  howpublished = {\url{https://www.tbank.ru/about/news/20072024-t-bank-opened-access-its-own-russian-language-language-model-weight-category-of-7-8-billion-parameters/}},
  note         = {Accessed: 2025-08-21}
}

@misc{yagpt,
  author       = {{Yandex}},
  title        = {YandexGPT 5 with reasoning mode},
  year         = {2025},
  howpublished = {\url{https://ya.ru/ai/gpt}},
  note         = {Accessed: 2025-07-30}
}