\documentclass{superfri}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[cal=rsfso]{mathalfa}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{makecell}
\usepackage{tabularx}
\usepackage{float}
\usepackage[normalem]{ulem}
\usepackage{graphicx}
\usepackage{xurl} 
\usepackage{lmodern}
\usepackage[unicode=true,linktocpage=true]{hyperref}
\usepackage{type1cm}

\bibliographystyle{splncs04}

\begin{document}
\raggedbottom

\author{D.\,A.~Grigoriev\footnote{\label{msu}Lomonosov Moscow State University, Moscow, Russia}\footnote{E-mail: dagrig14@yandex.ru}
\and D.\,I.~Chernyshev\footnoteref{msu}\footnote{E-mail: chdanorbis@yandex.ru}}

\title{RuWikiBench: Evaluating Large Language Models through encyclopedia article replication}

\maketitle{}

\begin{abstract}
In light of the growing interest in using large language models (LLMs) as tools for generating scientific texts,
the evaluation of their ability to produce encyclopedic content is becoming increasingly relevant.
However, for Russian-language materials this issue has not been sufficiently studied, and existing benchmarks do not cover key aspects of analytical work with sources.
This paper presents RuWikiBench - an open benchmark based on Ruwiki for evaluating the ability of large language models to reproduce Wikipedia-style articles,
built around three tasks:
selection of relevant sources, article structuring, and section generation.
The results popular open-source LLM evaluation show that even under ideal conditions, the best models do not always follow the expert logic of composing encyclopedic content:
even with a perfect source retrieval system, the models cannot reproduce the reference outline, and the quality of section generation shows almost no improvement with increase of model parameters.

\keywords{benchmark, Wikipedia, Ruwiki, large language model}
\end{abstract}

\section*{Introduction}
Modern large language models demonstrate impressive text generation results for wide range of topics and domains. 
However, their capabilities for working with scientific and encyclopedic materials remain understudied, particularly for Russian-language texts. 
Existing methods for model capability evaluation predominantly focus on standard linguistic tasks, without paying sufficient attention to analytical abilities when working with scientific texts. 
For the Russian language, this problem is especially relevant due to the limited availability of specialized evaluation tools.

There are many benchmarks covering various linguistic tasks for the Russian language. 
RussianSuperGlue \cite{rsglue} evaluates general language understanding and basic natural language processing tasks. 
MERA \cite{mera} provides unified testing conditions for models by compiling generation instructions for each task; however, the tasks themselves are oriented towards testing general comprehension. 
LIBRA \cite{libra} focuses on testing a model's ability to retain and retrieve information from a large context but is centered on short answers that do not require deep reasoning. 
Ru Arena General \cite{arena} focuses on pairwise model comparison rather than overall answer quality. 
Ping-Pong \cite{pp} evaluates the dialog abilities of models, which is important for interactive systems, 
but is not suitable for assessing the ability to conduct research and write coherent scientific-encyclopedic texts. 
At the same time, an entire class of tasks related to deep text analysis remains uncovered: creating detailed, structured, and factually accurate texts supported by a large number of sources.

The recent development of new agent capabilities, such as the emergence of the "Deep Research" function by OpenAI \cite{deepr} or the development of the universal Storm algorithm \cite{storm}, 
indicates a growing interest in conducting scientific research using large language models. 
This highlights the need to create new approaches for objectively evaluating the analytical capabilities of models. 
Existing benchmarks only partially address aspects critical for generating scientific-encyclopedic texts, 
such as the ability to extract relevant information from a set of documents, plan the structure of a future text, 
maintain text coherence and fluency, as well as ensure the factual consistency. One of the closest studies in this area is the ResearchArena \cite{resar} benchmark, 
which formalizes the construction of an academic review; however, it is more aimed at testing the models' ability 
to select and organize relevant information and does not address their ability to generate coherent scientific-encyclopedic texts.

This paper proposes an approach for evaluating large language model abilities to analyze and produce scientific-encyclopedic texts.
Our main contributions:
\begin{enumerate}
\item We collect "Ruwiki", a labeled dataset for generation of wikipedia-style articles in Russian language;
\item We propose new benchmark, RuWikiBench that measures large language model analytical capabilities in Russian language;
\item We evaluate the abilities of popular open-source large language models to generate Wikipedia-style articles.
\end{enumerate}

The code and data of this work have been made publicly available\footnote{\url{https://github.com/Nejimaki-Tori/WikiBench}}.

\section{Dataset collection}
To measure analytical capabilities of large language models we need a labeled corpus of texts that would contain various sources, 
the respective analysis of those sources and analysis-source mapping for factuality evaluation of claims. 
Wikipedia articles are the best candidates for such data because this genre simultaneously requires proving factual accuracy, acknowledging different perspectives, 
and expressing complex topics in common terms.

We utilize articles from Russian online encyclopedia "RuWiki" which distinguishing features are a large number of references to Russian-language sources, 
as well as stricter text filtering. These qualities ensure that the articles can be reliably verified and reproduced by human experts and therefore replicated by LLMs.

The data acquisition process included the following steps:
\begin{enumerate}
\item \textbf{Article Selection}: Articles on diverse topics containing a sufficient number of references to external sources were manually selected;
\item \textbf{Source Downloading}: For each article, the available sources it references were web-scraped;
\item \parbox[t]{0.9\textwidth}{\textbf{Splitting into Snippets}: To reproduce real Retrieval Augmented Generation (RAG) conditions, all texts were split into small fragments of approximately $\approx 600$ words in length.}
\end{enumerate}

\fig{width=0.8\linewidth}{figures/Source_extract.png}{Source extract}

\fig{width=0.8\linewidth}{figures/article_entities.png}{Main article entities}

\figref{figures/Source_extract.png} shows a brief schematic of the source text extraction process. The sources were downloaded using the Python module \texttt{newspaper3k}\footnote{\url{https://github.com/codelucas/newspaper}}.
A subset of "Ruwiki" articles \(B\) is taken as the initial corpus.
To extract article's HTML code we use BeautifulSoup4\footnote{\url{https://beautiful-soup-4.readthedocs.io/en/latest/}}.
The obtained text is structured by splitting it into fragments corresponding to nested headings (H1, H2, H3, etc.), which preserves both the substantive part of the article and its hierarchical organization.
Next, all external references cited in the "Notes" section are automatically extracted. Invalid links (e.g., 404 error) are excluded from further processing,
and the text associated with them is removed, leaving only those sources that are actually accessible.

\figref{figures/article_entities.png} illustrates the schematic breakdown of an article\footnote{\url{https://ru.ruwiki.ru/wiki/Python}} into key entities used in subsequent processing.
During the data processing stage, text filtering is performed to ensure its correct interpretation by the model.
Each footnote (e.g., [1], [2]) is matched to a specific link corresponding to one of the available sources.
This allows for precise identification of the link's position in the article text and its use for filtering.

\tab{tab:dataset}{Key characteristics of the collected dataset}{
    \begin{tabular}{lcc}
        \hline
        \textbf{Metric} & \textbf{RuWikiBench} & \textbf{ResearchArena} \\
        \hline
        Number of articles & 285 & 7,952 \\
        \hline
        Number of sources & 15,686 & 12,034,505 \\
        \hline
        Total number of snippets & 36,860 & - \\
        \hline
        Average outline size (headings) & 37 & - \\
        \hline
        Average section size (words) & 112 & - \\
        \hline
    \end{tabular}
}

Based on the valid links \(S_b^{\mathrm{filtered}}\), filtered sets of paragraphs \(A_b^{\mathrm{filtered}}\) and headings \(H_b^{\mathrm{filtered}}\) are formed.
That is, only content supported by the extracted sources is retained; everything else is removed.
Only sources for which a text \(t_q\) of at least 1500 characters could be retrieved are kept,
to filter out "noisy" responses from HTML pages such as errors (e.g., error 404) or blocking messages.
\(A_b^{\mathrm{filtered}}\) retains only those paragraphs that contain at least one link to a source for which text was successfully retrieved.
Similarly, \(H_b^{\mathrm{filtered}}\) is formed: it contains only those headings under which at least one paragraph remains.
The characteristics of the collected corpus are presented in \tabref{tab:dataset}.

\section{Evaluation Methodology}

To objectively assess the ability of language models to generate scientific and encyclopedic texts, it is necessary to replicate the real process of preparing encyclopedic content:
\begin{enumerate}
    \item \textbf{Selection of relevant sources}: The model is given the article title and a set of snippets, among which it must identify and rank materials relevant to the topic by their significance;
    \item \textbf{Article structure construction}: Based on the topic and selected sources, the model creates an outline with main sections in the Wikipedia-style;
    \item \textbf{Section generation}: Article materials are distributed across sections, after which a summary of the relevant materials is generated for each section.
\end{enumerate}
Each stage is evaluated independently of the previous ones, allowing for a quantitative measurement of the quality of each specific subtask.

\subsection{Selecting relevant sources}
One of the most effective search strategies \cite{rerank} is the preliminary generation of an expected result (description) based on the original query (article title) to create an expanded search query.
In our pipeline the description is generated in both Russian and English, to address available source text language diversity.
The queries in both languages are then combined into a single textual query for a BM25-based search system.

Experiments were conducted with two approaches to expanded query generation:
\begin{enumerate}
\item \textbf{Ground-truth query based on the title and second-level headings}: A ground-truth query which is used for direct evaluation of the models' ranking abilities.
We use LLaMa 3 70b model \cite{llama} to generate this query;
\item \textbf{Query generated from the title by the evaluated model}: Similar to real-world conditions, the LLM is fully responsible for the quality of the results and independently decides which search query to generate for BM25.
\end{enumerate}
Examples of generated descriptions are shown in \tabref{tab:cxx}.

\tab{tab:cxx}{Comparison of english-translated descriptions of the article “C++” in two variants}{
  \center
  \small
  \begin{tabular}{|p{0.16\linewidth}|p{0.74\linewidth}|}
  \hline
  \makecell{\textbf{Query} \\ \textbf{Variant}} & \makecell{\textbf{Text}} \\ 
  \hline

  \makecell{\textbf{Ground-truth}} & The article "C++" is an overview of the C++ programming language, its history, structure, and features. 
  It covers the main aspects of the language, including its standard library, differences from the C language, and further development. 
  In addition, the article contains examples of C++ programs, comparisons with alternative programming languages, as well as critical analysis and discussion of the influence of C++ on the development of programming and existing alternatives. 
  The article is intended for readers interested in the C++ language and its role in modern programming. \\ 
  \hline
  \makecell{\textbf{Generation} \\ \textbf{by title}}   & The article "C++" may be dedicated to the C++ programming language, one of the most popular and widely used programming languages in the world.
  The article may cover the basics of the language, its history, syntax and features, as well as its applications in various fields such as operating systems development, games, and web applications. 
  In addition, the article may include information about C++ standards and libraries, as well as its comparison with other programming languages. 
  The article can be useful both for beginner programmers and for experienced professionals who want to deepen their knowledge of C++. 
  It may also include code examples and practical tips on using C++ in real projects. \\ 
  \hline
  \end{tabular}
}

The documents selected by the BM25 query are sequentially passed to the large language model, which must classify each snippet as relevant (answer "yes") or non-relevant (answer "no").
To obtain numerical scores, we compare related wikipedia article titles of retrieved documents to title of article we seek to generate.
The logarithmic probability of the tokens in the model's response is taken: if the answer is positive, the probability $P(\text{yes})$ is used; if negative, $1 - P(\text{no})$ is used.
This approach allows ranking the retrieved documents by the model's confidence in their relevance: the higher the probability, the higher the model's confidence in the response, and the higher the document is ranked in the results.

\subsection{Article structure planning}
First, each text fragment (snippet) from the reference article source is converted by embedding model.
Then, the snippets are clustered into potential section contents.
To guarantee reproducibility KMeans algorithm is applied with the number of clusters equal to the number of second-level headings in the reference outline, and the centroids are initialized with the vector representations of these headings.

Next, five snippets closest to the cluster center are selected. This is done to reduce the influence of less relevant snippets on the final outline.
The generation of mini-outlines for sections is carried out taking into account two key parameters:
the context window size (to account for references and the overall semantics of the document) and two generation modes - directly from the texts and through preliminary generation of a brief cluster description.
These generation modes enable different levels of abstraction:
the direct mode preserves details with raw data, while the mode via preliminary cluster description
improves consistency and reduces information duplication.
At the final stage, all mini-outlines are combined into the final structured article outline.

\subsection{Section generation}
For each article section, we extract all snippets that were indicated as sources for the reference text of the section.
These snippets are again converted into embeddings, and a pairwise similarity matrix is constructed as the product \(E \times E^\top\).
Elements with a similarity value above the threshold of 0.8 (empirically determined) are considered semantically close and are grouped together to avoid redundant repetitions during generation (e.g., when different sources paraphrase the same information).
For each such semantic group, a hierarchical representation is built: the first five texts are taken, and a brief description is generated based on them.
This description is then supplemented using the next five texts, and so on, until a complete compressed representation of the group is obtained.
Thus, only a set of brief descriptions remains - the most important information without excessive repetition.
After this, the text of the section is generated based on the obtained group descriptions using a hierarchical summarization method \cite{hier}.

\section{Benchmark parameters}
Below is a description of all data, models, hyperparameters, and procedures used to ensure reproducibility and analysis.

\subsection{Generation parameters}
For all models, unless otherwise specified, the same generation parameters were used: temperature - 0.01, repetition penalty - 1.0, and top\_p - 0.9.

\subsection{Relevant source selection}
Snippet indexing was performed using BM25\footnote{\url{https://github.com/xhluca/bm25s}} across the entire corpus of collected snippets without hyperparameter tuning (default values).
For each relevant document, two non-relevant documents were selected (ratio 1:2) - this was done to improve evaluation robustness.

\subsection{Article structure planning}
To build snippet embeddings we used \texttt{sergeyzh/BERTA}\footnote{\url{https://huggingface.co/sergeyzh/BERTA}} model.
Two context window options were considered: either a zero window (only the snippet itself) or one neighboring snippet to the left and right to expand the context.
Heading similarity with reference headings was compared using cosine similarity: semantic correspondence was prioritized over exact wording or heading level.
The comparison was performed against the cleaned article structure: all headings whose sections consisted entirely of text without downloadable sources were removed from the preprocessed text.

\section{Metrics}

Within the benchmark, two groups of metrics are considered: ranking metrics, which assess how well the model selects relevant sources, and text similarity metrics, which measure how closely the generated content matches the reference.

\subsection{Ranking Metrics}

To evaluate the quality of the source list, we use $\mathrm{NDCG@K}$ \cite{ndcg} and $\mathrm{R\text{-}Precision}$ \cite{rprecision}:
\begin{equation}\label{ndcg}
\mathrm{NDCG@K}= \frac{\mathrm{DCG@K}}{\mathrm{IDCG@K}},
\end{equation}

\begin{equation}\label{dcg}
\mathrm{DCG@K}= \sum_{i=1}^{K} \frac{\mathrm{rel}_i}{\log_2(i+1)},
\end{equation}

\begin{equation}\label{idcg}
\mathrm{IDCG@K}= \sum_{i=1}^{K} \frac{\mathrm{rel}^{\mathrm{IDEAL}}_i}{\log_2(i+1)},
\end{equation}

\begin{equation}\label{rpr}
\mathrm{R\text{-}Precision}= \frac{\sum_{i=1}^{R} \mathrm{rel}_i}{R},
\end{equation}

where $\mathrm{rel}_i\in\{0,1\}$ is the indicator of relevance for the document at position $i$; $\mathrm{rel}^{\mathrm{IDEAL}}_i$ is the same quantity in the ideal (fully sorted) ranking;
$R$ is the total number of relevant documents for the given query.

\subsection{Text Similarity Metrics}
The quality of generated sections and headings is evaluated with \textbf{BERTScore} \cite{bertscore}:

\begin{equation}\label{recall}
R_{\mathrm{BERT}}= \frac{1}{|x|}\sum_{x_i\in x}\max_{\hat{x}_j\in\hat{x}} x_i^\top \hat{x}_j,
\end{equation}

\begin{equation}\label{precision}
P_{\mathrm{BERT}}= \frac{1}{|\hat{x}|}\sum_{\hat{x}_j\in\hat{x}}\max_{x_i\in x} x_i^\top \hat{x}_j,
\end{equation}

\begin{equation}\label{f}
F_{\mathrm{BERT}}= \frac{2\,P_{\mathrm{BERT}}\,R_{\mathrm{BERT}}}{P_{\mathrm{BERT}} + R_{\mathrm{BERT}}},
\end{equation}

where $x$ is the reference text, $\hat{x}$ is the generated text; each sentence is encoded using the model\footnote{\url{https://huggingface.co/sergeyzh/BERTA}}, after which cosine similarity is computed.

ROUGE-\allowbreak L and BLEU were also considered for evaluating section generations.

\textbf{ROUGE-L} \cite{rouge} is based on the length of the longest common subsequence (LCS) between the generated summary $S$ and the reference $R$:
\begin{equation}\label{r_p}
  \text{Precision} = \frac{\mathrm{LCS}(S,R)}{|S|},
\end{equation}
\begin{equation}\label{r_r}
  \text{Recall} = \frac{\mathrm{LCS}(S,R)}{|R|},
\end{equation}
\begin{equation}\label{rouge}
  \text{ROUGE‑L} = \frac{2\;\text{Precision}\;\cdot\;\text{Recall}}{\text{Precision} + \text{Recall}},
\end{equation}

\textbf{BLEU} \cite{bleu} is an n-gram precision metric with a brevity penalty:
\begin{equation}\label{bleu}
\mathrm{BLEU}_N = \mathrm{BP}\cdot \exp\!\left(\sum_{n=1}^{N} w_n \log p_n\right),
\end{equation}
where $p_n$ is the precision for $n$-grams, $w_n$ are the weights, and $\mathrm{BP}$ is the brevity penalty.


\section{Experiments}

\subsection{Models}
The experiments used the following large language models:
RuadaptQwen2.5-\allowbreak 7B-\allowbreak Lite-\allowbreak Beta \cite{ruadapt},
RuadaptQwen3-\allowbreak 32B-\allowbreak Instruct-\allowbreak v2 \cite{ruadapt}, 
DeepSeek V3 \cite{deepseek}, 
Qwen3-\allowbreak 235B-\allowbreak A22B \cite{qwen3}, 
tpro \cite{tpro} and yagpt5lite \cite{yagpt}.
In all tables, models are grouped by size, and the best results within each group are highlighted.

\subsection{Results}
\tabref{tab:query_ref} and \tabref{tab:query_def} present the results of source selection evaluation.
As the baseline we use BM25 retrieval without model-based reranking.
In the first case (\tabref{tab:query_ref}), where a pre-generated ground-truth search query was used for all models,
the best results were achieved by DeepSeek V3, which indicates a strong ability to select relevant documents.
In the second experiment (\tabref{tab:query_def}), where the query was based solely on the article title, tpro achieved the best results.
The experiment showed that LLM query generation is not inferior in ranking quality to the reference setup.
Presumably, this is because (as shown in \tabref{tab:cxx}) the queries turn out to be very similar to ground-truth:
LLMs have knowledge of Wikipedia’s typical article structure from training and can therefore connect relevant concepts in the required format.

\tab{tab:query_ref}{Results of pure ranking ability evaluation}{
\centering
  \begin{tabular}{l|c|c}
    \hline
    \textbf{Model} & \textbf{NDCG} & \textbf{R-Pr} \\
    \hline
    baseline (bm25)                                                  & 88.81 & 62.51 \\
    \hline
    \textbf{DeepSeek V3}                                             & \uline{\textbf{95.42}} & \uline{\textbf{83.86}} \\
    Qwen3-\allowbreak 235B-\allowbreak A22B                          & 94.49 & 82.42 \\
    \hline
    RuadaptQwen3-\allowbreak 32B-\allowbreak Instruct-\allowbreak v2 & 95.25 & 81.81 \\
    tpro                                                             & \uline{95.42} & \uline{83.53} \\
    \hline
    RuadaptQwen2.5-7B-\allowbreak Lite-\allowbreak Beta              & 88.26 & 62.26 \\
    yagpt5lite                                                       & \uline{90.35} & \uline{77.66} \\
    \hline
  \end{tabular}
}

\tab{tab:query_def}{Results of evaluating BM25 query-generation ability}{
\centering
  \begin{tabular}{l|cc|cc}
    \hline
    \multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c|}{\textbf{BM25}} & \multicolumn{2}{c}{\textbf{Rerank}} \\
    & \textbf{NDCG} & \textbf{R-Pr} & \textbf{NDCG} & \textbf{R-Pr} \\
    \hline
    DeepSeek V3                                                      & 88.39 & 60.65 & \uline{95.67} & \uline{83.07} \\
    Qwen3-235B-A22B                                                  & \uline{89.17} & \uline{62.98} & 94.90 & 81.96 \\
    \hline
    RuadaptQwen3-\allowbreak 32B-\allowbreak Instruct-\allowbreak v2 & 85.39 & 52.80 & 95.82 & 81.62 \\
    \textbf{tpro}                                                    & \uline{\textbf{90.61}} & \uline{\textbf{65.07}} & \uline{\textbf{96.06}} & \uline{\textbf{83.37}} \\
    \hline
    RuadaptQwen2.5-7B-\allowbreak Lite-\allowbreak Beta              & \uline{88.81} & \uline{62.51} & 88.23 & 60.96 \\
    yagpt5lite                                                       & 86.59 & 57.98 & \uline{90.27} & \uline{77.65} \\
    \hline
  \end{tabular}
}

Overall, the models show fairly high performance at this stage, which may be due to the fact that an article title reflects its content well.
In the best cases, up to 80\% of the documents in the sample are relevant, which implies there is significant room for for further improvement.

\tab{tab:outline_res}{Results of outline generation}{
\centering
  \begin{tabular}{l|c|c|c}
    \hline
    \multirow{2}{*}{\textbf{Model}} & \multicolumn{3}{c}{\textbf{Mean BERTScore F1}} \\
    \cline{2-4}
    & \multicolumn{2}{c|}{\textbf{Direct}} & \multirow{2}{*}{\textbf{Description}} \\
    \cline{2-3}
    & \textbf{no neighbors} & \makecell{\textbf{one neighbor}} & \\
    \hline
    \textbf{DeepSeek V3}                                             & \uline{\textbf{63.51}} & \uline{\textbf{62.93}} & \uline{\textbf{65.50}} \\
    Qwen3-\allowbreak 235B-\allowbreak A22B                          & 60.86 & 59.06 & 62.66 \\
    \hline
    RuadaptQwen3-\allowbreak 32B-\allowbreak Instruct-\allowbreak v2 & 60.12 & \uline{60.04} & \uline{62.91} \\
    tpro                                                             & \uline{60.32} & 59.09 & 60.75 \\
    \hline
    RuadaptQwen2.5-7B-\allowbreak Lite-\allowbreak Beta              & \uline{60.03} & 58.21 & \uline{61.58} \\
    yagpt5lite                                                       & 59.72 & \uline{60.07} & 60.25 \\
    \hline
  \end{tabular}
}

\tabref{tab:outline_res} presents the results of article structure planning.
Direct - generation from the cluster snippets, with neighbor context as indicated; Description - generation via a preliminary description of all cluster elements.
The results show that with preliminary description generation, all models consistently improve in quality.
RuadaptQwen3 shows the largest gain, rising to second place and effectively matching the results of the larger model,
Qwen3-\allowbreak 235B-\allowbreak A22B. DeepSeek V3 remains the leader, showing a substantial margin over the others. At the bottom in quality are RuadaptQwen2.5-7B-\allowbreak Lite-\allowbreak Beta and yagpt5lite.
At the same time, yagpt5lite, with only 8 billion parameters, delivers results comparable to a 32-billion-parameter model.
\tabref{tab:outline} shows a comparison of a small excerpt of the reference and generated outlines.
A common issue across all models was excessive heading hierarchy depth.
On “Ruwiki”, headings were rarely deeper than level three; however, the models often created fourth- and fifth-level headings, implying that all information belongs in one large section,
even though it may differ somewhat in meaning and, in the original outline, would correspond to unrelated headings.

\tab{tab:outline}{Comparison of two englis-translated article outlines}{
\centering
  \begin{multicols}{2}
    \textbf{Generated} \\
    \small\setlength{\parskip}{2pt}\setlength{\parindent}{0pt}
    \texttt{\#} Introduction to Python\\
    \texttt{\#\#} Language Overview\\
    \texttt{\#\#\#} History and Key Aspects\\
    \texttt{\#\#\#\#} Main Features and Implementations\\
    \texttt{\#} Python Basics\\
    \texttt{\#\#} Syntax and Semantics\\
    \texttt{\#\#\#} Data Types and Structures\\
    \texttt{\#\#\#\#} Numbers, Lists, Dictionaries \\ and Object-Oriented Programming\\
    \texttt{\#} Advanced Python Topics\\
    \texttt{\#\#} Flow Control and Multithreading\\
    \dots
    \columnbreak
    \\ \textbf{Reference} \\
    \small\setlength{\parskip}{2pt}\setlength{\parindent}{0pt}
    \texttt{\#} Python\\
    \texttt{\#\#} History\\
    \texttt{\#\#} Concept and Philosophy\\
    \texttt{\#\#} Portability\\
    \texttt{\#\#} Data Types and Structures\\
    \texttt{\#\#} Syntax and Semantics\\
    \texttt{\#\#\#} Indentation System\\
    \texttt{\#\#\#} Expressions\\
    \texttt{\#\#\#} Names\\
    \texttt{\#\#\#} Documentation Strings\\
    \texttt{\#\#} Programming Paradigms\\
    \dots
  \end{multicols}
}

\tabref{tab:secs} reports the evaluation of section-generation quality. The difference between model performance may be considered marginal, but this is due to the sensitivity of the metric used.
Sections for which the algorithm did not select a single relevant snippet were excluded from the final metrics. The best overall results
were demonstrated by Qwen3-\allowbreak 235B-\allowbreak A22B; however, in terms of ROUGE-\allowbreak L and BLEU, RuadaptQwen3-\allowbreak 32B-\allowbreak Instruct-v2 shows better structural consistency and
greater phrase overlap with the reference.
The yagpt5lite model performs above average, especially on BLEU, at a much smaller size, whereas tpro shows the lowest values across all metrics.

\tab{tab:secs}{Results of section generation}{
\centering
  \begin{tabular}{l|c|c|c}
    \hline
    \textbf{Model} & \textbf{Mean F1} & \textbf{Mean ROUGE-L} & \textbf{Mean BLEU} \\
    \hline
    DeepSeek V3                                                               & 53.48 & 14.34 & 2.81 \\
    Qwen3-\allowbreak 235B-\allowbreak A22B                                   & \uline{\textbf{53.74}} & \uline{14.63} & \uline{3.07} \\
    \hline
    \textbf{RuadaptQwen3-\allowbreak 32B-\allowbreak Instruct-\allowbreak v2} & 53.21 & \uline{\textbf{15.46}} & \uline{\textbf{3.40}} \\
    tpro                                                                      & 53.15 & 13.58 & 2.27 \\
    \hline
    RuadaptQwen2.5-7B-\allowbreak Lite-\allowbreak Beta                       & 52.99 & 12.29 & 2.11 \\
    yagpt5lite                                                                & \uline{53.43} & \uline{14.85} & \uline{3.16} \\
    \hline
  \end{tabular}
}

\tab{tab:sec_cov}{Comparison of english-translated texts of two sections}{
\centering
  \begin{tabular}{|p{0.2\linewidth}|p{0.7\linewidth}|}
    \hline
    \makecell{\textbf{Model}} & \makecell{\textbf{Text}} \\ \hline
    \textbf{DeepSeek V3} & \textbf{COVID-19 is an infectious disease}, \dots which \textbf{led to a global pandemic} that began in 2020.
    Initially presenting with respiratory symptoms such as cough, fever, and shortness of breath, the disease can cause severe complications, \dots
    The virus is highly transmissible, is presumed to have a zoonotic origin, and spread rapidly worldwide.
    To control the pandemic, the WHO recommends vaccination, mask-wearing, social distancing, and hand hygiene, with vaccine effectiveness against the original strain reaching 85\% or higher.
    Although COVID-19 in children more often has a mild course, severe cases are possible, including multisystem inflammatory syndrome. \\ 
    \hline
    \textbf{yagpt5lite} & \textbf{COVID-19 is a pandemic} caused by the novel coronavirus SARS-CoV-2.
    As of January 14, 2022, the WHO had confirmed about \textbf{318,648,834} cases of COVID-19 worldwide, including \textbf{5,518,343} deaths.
    The first COVID-19 vaccine was introduced in December 2020. On December 2, 2020, the United Kingdom became the first country to approve the Pfizer-BioNTech (BNT162) vaccine, which the WHO authorized for emergency use.
    SARS-CoV-2 is considered more contagious than SARS-CoV and quickly spread around the world after several infection cases in Wuhan, China.
    The pathogenesis of SARS-CoV-2 is associated with inflammatory responses that adversely affect the lungs and cause symptoms such as cough, fever, general malaise, shortness of breath, and respiratory failure. \\ 
    \hline
  \end{tabular}
\label{tab:secs_com}
}

For a qualitative evaluation of section-generation quality, we consider the introductory parts of the article “COVID19”
produced by DeepSeek V3 and yagpt5lite, respectively, shown in \tabref{tab:secs_com}.
Despite some semantic inaccuracies (for example, the statement “COVID-19 is a pandemic,” whereas in reality it is a disease),
yagpt5lite demonstrates a quite solid result. Its text falls short of DeepSeek V3’s version in terms of coverage and systematic exposition,
but contains more numerical data and concrete facts. At the same time, the material generated by DeepSeek V3 tends to resemble an encyclopedic article,
whereas the yagpt5lite version is closer in style to a technical report on the disease.

\section*{Conclusion}
This paper proposes RuWikiBench benchmark for evaluating the analytical capabilities of large language models in generating scientific and encyclopedic texts in Russian. 
The core of the proposed evaluation system is a three-stage process, consisting of three independent systems that naturally arise when creating articles on a specific topic. 
Relying on a filtered "Ruwiki" corpus and a clearly defined evaluation methodology, the proposed benchmark establishes a foundation for further research in the application of language models to the 
task of generating scientific and encyclopedic text in Russian language.

Experiments showed that with a fixed search query, DeepSeek V3 demonstrates the best source selection quality, significantly outperforming BM25 without reranking. 
At the structure planning stage, it was found that adding a preliminary cluster description consistently improves the quality of outlines for all models, 
including DeepSeek V3, which demonstrated the best understanding of the process. All models showed comparable section generation quality; 
however, RuadaptQwen3-\allowbreak 32B-\allowbreak Instruct-\allowbreak v2 leads in ROUGE-L and BLEU metrics, indicating a text structure more consistent with the reference. 
The work shows that models possess significant potential, but their reliable application requires further development of methods for analyzing and structuring review materials.

\section*{Acknowledgements}
The study was supported by grant No. 25-11-00191 from the Russian Science Foundation.
The work was carried out using the supercomputer "MSU-270" of the Lomonosov Moscow State University.

\openaccess

\bibliography{bitex}

\end{document}